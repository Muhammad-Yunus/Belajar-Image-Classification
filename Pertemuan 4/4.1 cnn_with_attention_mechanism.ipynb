{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Attention Mechanism in CNN\n",
    "- Intro To Attention Mechanism\n",
    "- Experiment adding Attention Mechanism to CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Intro To Attention Mechanism\n",
    "- <font color=\"orange\">Attention</font> is, to some extent, motivated by how we <font color=\"orange\">pay visual attention</font> to,\n",
    "    - <font color=\"cyan\">different regions</font> of an <font color=\"orange\">image</font> (*vision task*), or \n",
    "    - <font color=\"cyan\">correlate words</font> in one <font color=\"orange\">sentence</font> (*language task*).<br><br>\n",
    "<img src=\"resource/shiba-example-attention.png\" width=\"700px\"><br><br>\n",
    "<img src=\"resource/sentence-example-attention.png\" width=\"700px\"><br>\n",
    "- Humans can naturally and effectively find <font color=\"orange\">salient regions</font> in complex scenes. \n",
    "- Attention mechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system.\n",
    "- An attention mechanism can be regarded as a <font color=\"orange\">dynamic weight adjustment</font> process based on <font color=\"orange\">features</font> of the input image.<br><br>\n",
    "- Historical <font color=\"orange\">timeline developments</font> in attention in computer vision : <br><br>\n",
    "<img src=\"resource/attension-dev.png\" width=\"900px\"><br><br>\n",
    "- Attention mechanisms can be <font color=\"orange\">categorised</font> according to data domain : <br><br>\n",
    "<img src=\"resource/attention-category.png\" width=\"500px\"><br><br>\n",
    "    - <font color=\"cyan\">Channel attention</font> generate <font color=\"orange\">attention mask</font> across the <font color=\"orange\">channel domain</font> and use it to select important channels. \n",
    "    - <font color=\"cyan\">Spatial attention</font> generate <font color=\"orange\">attention mask</font> across <font color=\"orange\">spatial domains</font> and use it to select important spatial regions or predict the most relevant spatial position directly.<br><br>\n",
    "    - <font color=\"cyan\">Channel & spatial attention</font> predict <font color=\"orange\">channel</font> and <font color=\"orange\">spatial attention masks</font> separately or generate a joint 3-D channel, height, width attention mask directly and use it to select <font color=\"orange\">important features</font>.\n",
    "    - <font color=\"cyan\">Temporal attention</font> generate <font color=\"orange\">attention mask</font> in <font color=\"orange\">time</font> and use it to select <font color=\"orange\">key frames</font>.\n",
    "    - <font color=\"cyan\">Spatial & temporal attention</font> compute <font color=\"orange\">temporal</font> and <font color=\"orange\">spatial attention masks</font> separately or produce a joint <font color=\"orange\">spatiotemporal attention</font>.\n",
    "    - <font color=\"cyan\">Branch attention</font> generate <font color=\"orange\">attention mask</font> across the different branches and use it to select <font color=\"orange\">important branches</font>.<br><br>\n",
    "<img src=\"resource/Attention-visual.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1.1 Channel Attention - SENet\n",
    "- <font color=\"cyan\">SENet</font> pioneered channel attention.\n",
    "- The core of <font color=\"cyan\">SENet</font> is *<font color=\"orange\">squeeze-and-excitation</font>* block which is used to collect <font color=\"orange\">global information</font>, capture <font color=\"orange\">channelwise relationships</font>, and improve <font color=\"orange\">representation ability</font>.\n",
    "- <font color=\"orange\">SE blocks</font> are divided into two parts, a <font color=\"cyan\">squeeze</font> module and an <font color=\"cyan\">excitation</font> module.\n",
    "    - <font color=\"orange\">Global information</font> is collected in the <font color=\"orange\">squeeze</font> module by <font color=\"cyan\">global average pooling (GAP)</font>.\n",
    "    - The <font color=\"orange\">excitation</font> module captures <font color=\"orange\">channel-wise relationships</font> and outputs an <font color=\"orange\">attention vector</font> by using <font color=\"cyan\">fully-connected layers</font> and <font color=\"cyan\">non-linear activation layers</font> (ReLU and sigmoid).\n",
    "    - Then, <font color=\"orange\">each channel</font> of the <font color=\"cyan\">input feature</font> is scaled by <font color=\"orange\">multiplying</font> the corresponding element in the <font color=\"cyan\">attention vector</font>.<br><br>\n",
    "        <table cellspacing=\"0\" cellpadding=\"0\" style=\"border:none;\">\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td><img src=\"resource/attention-se-block.png\" width=\"250px\"></td>\n",
    "                    <td>GAP = global average pooling<br>FC = fully-connected layer<br><br><br><img src=\"resource/attention-se-block-2.png\" width=\"500px\"><br></td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table><br><br>\n",
    "- <font color=\"orange\">SE blocks</font> play the role of <font color=\"orange\">emphasizing important channels</font> while <font color=\"orange\">suppressing noise</font>.\n",
    "- However, SE blocks have shortcomings. \n",
    "    - In the <font color=\"orange\">squeeze</font> module, <font color=\"orange\">global average pooling</font> is <font color=\"cyan\">too simple</font> to capture complex global information. \n",
    "    - In the <font color=\"orange\">excitation</font> module, <font color=\"orange\">fully-connected layers</font> <font color=\"cyan\">increase the complexity</font> of the model.\n",
    "    - later works attempt to improve the outputs of the squeeze module (e.g., GSoP-Net),\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1.2 Channel Attention - GSoP-Net\n",
    "- GSoP-Net improve the squeeze module by using a global second-order pooling (GSoP) block.\n",
    "- Like an SE block, a GSoP block also has a squeeze module and an excitation module.\n",
    "- The squeeze module a GSoP block, \n",
    "    - Firstly reduces the number of channels using a convolution.\n",
    "    - And then computes a covariance matrix for the different channels to obtain their correlation.\n",
    "- The excitation module a GSoP block,\n",
    "    - Compute row-wise convolution to maintain structural information and output a vector.\n",
    "    - Then a fullyconnected layer and a sigmoid function are applied to get a attention vector<br><br>\n",
    "        <table cellspacing=\"0\" cellpadding=\"0\" style=\"border:none;\">\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td><img src=\"resource/attention-gsop-block.png\" width=\"250px\"></td>\n",
    "                    <td>Cov pool = Covariance pooling<br>RW Conv = row-wise convolution</td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1.3 Spatial Attention - Self-Attention Based Method\n",
    "- Self-attention was proposed and has had great success in the field of natural language processing (NLP).\n",
    "- Recently, it has also shown the potential to become a dominant tool in computer vision.\n",
    "- Typically, selfattention is used as a spatial attention mechanism to capture global information.\n",
    "- Due to the localisation of the convolutional operation, CNNs have inherently narrow receptive fields, which limits the ability of CNNs to understand scenes globally.\n",
    "- self-attention first computes the queries, keys, and values Q, K, V by linear projection and reshaping operations\n",
    "<img src=\"resource/attention-self-attention.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Experiment adding Attention Mechanism to CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️⚠️ *Please open this notebook in Google Colab* by click below link ⚠️⚠️⚠️<br><br>\n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-Image-Classification/blob/main/Pertemuan%203/3.1%20intro_to_cnn.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><br><br><br>\n",
    "- Click `Connect` button in top right Google Colab notebook,<br>\n",
    "<img src=\"resource/cl-connect-gpu.png\" width=\"250px\">\n",
    "- If connecting process completed, it will turn to something look like this<br>\n",
    "<img src=\"resource/cl-connect-gpu-success.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check GPU connected into Colab environment is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All code remain same with <font color=\"orange\">Pertemuan 1</font>, but different in designing model part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import gdown\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# clear output cell\n",
    "display.clear_output()\n",
    "\n",
    "print(f\"torch : {torch.__version__}\")\n",
    "print(f\"torch vision : {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'MNIST' # the dataset name\n",
    "DATASET_NUM_CLASS = 10 # number of class in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default using gdrive_id Dataset `mnist_dataset.zip` (1-FfwJrllyHofQwIbMb_IxAkxnfMGSFmR)\n",
    "gdrive_id = '1-FfwJrllyHofQwIbMb_IxAkxnfMGSFmR' # <-----  ⚠️⚠️⚠️ USE YOUR OWN GDrive ID FOR CUSTOM DATASET ⚠️⚠️⚠️\n",
    "\n",
    "# download zip from GDrive\n",
    "url = f'https://drive.google.com/uc?id={gdrive_id}'\n",
    "gdown.download(url, DATASET_NAME + \".zip\", quiet=False)\n",
    "\n",
    "# unzip dataset\n",
    "!unzip {DATASET_NAME}.zip -d {DATASET_NAME}\n",
    "\n",
    "# clear output cell\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load MNIST Dataset\n",
    "    - <font color=\"orange\">DONT FLATTEN THE INPUT IMAGE ON DATA LOADER</font>,\n",
    "    - WE WILL FEED 2D 28x28 MNIST DIGIT IMAGE DATA INTO MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Dataset class\n",
    "# it's just helper to load image dataset using OpenCV and convert to pytorch tensor\n",
    "# also doing a label encoding using one-hot encoding\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = sorted([file for file in os.listdir(root_dir) if file.lower().endswith('.png')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read image from corresponding .png file\n",
    "        image_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = cv2.imread(image_path)  # Load image using OpenCV\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert BGR to GRAY\n",
    "        image = torch.from_numpy(image).to(torch.float32)  # Convert NumPy array to PyTorch tensor\n",
    "        image = image.view(1, 28, 28) # reshape loaded MNIST image into 1x28x28 format (required by the model)\n",
    "\n",
    "        # Read label from corresponding .txt file\n",
    "        label_path = os.path.splitext(image_path)[0] + \".txt\"\n",
    "        with open(label_path, 'r') as label_file:\n",
    "            label = int(label_file.read().strip())  # Assuming labels are integers\n",
    "\n",
    "        # Apply one-hot encoding into label\n",
    "        labels_tensor = torch.tensor(label)\n",
    "        one_hot_encoded = F.one_hot(labels_tensor, num_classes=DATASET_NUM_CLASS).to(torch.float32)\n",
    "\n",
    "        return image, one_hot_encoded\n",
    "\n",
    "\n",
    "\n",
    "# instantiate dataset\n",
    "# in here the image dataset is not loaded yet\n",
    "# we only read all image files names in fataset folder\n",
    "all_train_dataset = CustomDataset(root_dir=f'{DATASET_NAME}/dataset/train')\n",
    "test_dataset = CustomDataset(root_dir=f'{DATASET_NAME}/dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"All Train Dataset : {len(all_train_dataset)} data\")\n",
    "print(f\"Test Dataset : {len(test_dataset)} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'all_train_dataset' into 'train' and 'validation' set using `random_split()` function\n",
    "train_dataset, validation_dataset = random_split(all_train_dataset, [50000, 10000])\n",
    "\n",
    "print(f\"Train Dataset : {len(train_dataset)} data\")\n",
    "print(f\"Validation Dataset : {len(validation_dataset)} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we use model defined <font color=\"orange\">'3.3 cnn_wuth_batch_normalization.ipynb'</font>.\n",
    "    - With additional <font color=\"orange\">Attention Mechanism</font><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    # Convolutional layers with Batch Normalization\n",
    "    nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3, padding=2, bias=False),\n",
    "    nn.BatchNorm2d(num_features=12, affine=True), \n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=12, out_channels=24, kernel_size=6, stride=2, padding=2, bias=False),\n",
    "    nn.BatchNorm2d(num_features=24, affine=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=24, out_channels=32, kernel_size=6, stride=2, padding=2, bias=False),\n",
    "    nn.BatchNorm2d(num_features=32, affine=True),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    # Flatten layer\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    # Fully connected layers with Batch Normalization\n",
    "    nn.Linear(in_features=32 * 7 * 7, out_features=200, bias=False),\n",
    "    nn.BatchNorm1d(num_features=200, affine=True), \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.6),\n",
    "    nn.Linear(in_features=200, out_features=10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ").to(device)\n",
    "\n",
    "# Iterate over model to find BatchNorm layers and modify them\n",
    "for layer in model:\n",
    "    if isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.BatchNorm1d):\n",
    "        # Set weight to 1 (disabling scaling)\n",
    "        with torch.no_grad():\n",
    "            layer.weight.fill_(1.0)  # Set weight to 1\n",
    "        # Freeze the weight from being updated\n",
    "        layer.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer, loss function & metric\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To run training process, we can use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Add progress bar for training loop\n",
    "    progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs = inputs.to(device) # move inputs to device\n",
    "        labels = labels.to(device) # move labels to device\n",
    "\n",
    "        # resets the gradients of all the model's parameters before the backward pass\n",
    "        optimizer.zero_grad()\n",
    "        # pass 2D 28x28 input tensor to CNN model\n",
    "        outputs = model(inputs)\n",
    "        # calc loss value\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # computes the gradient of the loss with respect to each parameter in model\n",
    "        loss.backward()\n",
    "        # adjust model parameter\n",
    "        optimizer.step()\n",
    "        # sum loss value\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate correct & total prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels.argmax(1)).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Update progress bar description with current loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate average training loss\n",
    "    average_train_loss = running_loss / len(train_loader.dataset)\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    return average_train_loss, train_accuracy\n",
    "\n",
    "def validate(model, val_loader, loss_function):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Add progress bar for validation loop\n",
    "    progress_bar = tqdm(val_loader, desc='Validating', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device) # move inputs to device\n",
    "            labels = labels.to(device) # move labels to device\n",
    "\n",
    "            # pass 2D 28x28 input tensor to CNN model\n",
    "            outputs = model(inputs)\n",
    "            # calc loss value\n",
    "            loss = loss_function(outputs, labels)\n",
    "            # sum loss value\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate correct & total prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels.argmax(1)).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            # Update progress bar description with loss\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    average_val_loss = running_loss / len(val_loader.dataset)\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "    return average_val_loss, val_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This is a training loop for selected Epoch\n",
    "# each epoch will process all training and validation set, chunked into small batch size data\n",
    "# then measure the loss & accuracy of training and validation set\n",
    "NUM_EPOCH = 10      # you can change this value\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCH}\")\n",
    "\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, loss_function)\n",
    "    val_loss, val_accuracy = validate(model, validation_loader, loss_function)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy * 100)  # convert to percentage\n",
    "    val_accuracies.append(val_accuracy * 100)  # convert to percentage\n",
    "\n",
    "    print(f\"Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot Loss and Accuracy of Training vs Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Loss & Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, NUM_EPOCH + 1))\n",
    "\n",
    "# Plotting loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate Model, find Precision, Recal each class data, measure accuracy and compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# define evaluate function for test set\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Add progress bar for validation loop\n",
    "    progress_bar = tqdm(test_loader, desc='Evaluating', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # iterate over all batched test set\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device) # move inputs to device\n",
    "            labels = labels.to(device) # move labels to device\n",
    "\n",
    "            # pass 2D 28x28 input tensor to CNN model\n",
    "            outputs = model(inputs)\n",
    "            # get prediction\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # collect all labels & preds\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Evaluation on test set\n",
    "all_labels, all_preds = evaluate(model, test_loader)\n",
    "all_labels = np.argmax(all_labels, axis=1)\n",
    "\n",
    "# Calculate classification report\n",
    "labels = [str(i) for i in range(DATASET_NUM_CLASS)]\n",
    "print(classification_report(all_labels, all_preds, target_names=labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'trained_cnn_model.pt')\n",
    "\n",
    "# Download the model file\n",
    "from google.colab import files\n",
    "files.download('trained_cnn_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    ">## Discussion\n",
    ">- It looks like dropout not help much to reduce overfitting,\n",
    ">- It's also has negative impact by reducing training accuracy.\n",
    ">- Now we will try to learn new possibility to adopt other regularization technique.\n",
    ">- It's called <font color=\"cyan\">Batch Normalization</font>\n",
    ">."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Open <font color=\"orange\">'3.3 cnn_with_batch_normalization.ipynb'</font> in Google Colab to learn more...<br> \n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-Image-Classification/blob/main/Pertemuan%203/3.3%20cnn_with_batch_normalization.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________\n",
    "<br><br><br>\n",
    "# Source\n",
    "- https://lilianweng.github.io/posts/2018-06-24-attention/?ref=blog.paperspace.com\n",
    "- https://link.springer.com/content/pdf/10.1007/s41095-022-0271-y.pdf\n",
    "- https://www.researchgate.net/figure/Before-inputting-the-SE-attention-mechanism-left-colorless-figure-C-the-importance-of_fig1_366512193\n",
    "- https://www.digitalocean.com/community/tutorials/attention-mechanisms-in-computer-vision-cbam"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

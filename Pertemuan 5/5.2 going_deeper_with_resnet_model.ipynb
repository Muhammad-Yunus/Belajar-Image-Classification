{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Going deeper with ResNet Model\n",
    "\n",
    "⚠️⚠️⚠️ *Please open this notebook in Google Colab* by click below link ⚠️⚠️⚠️<br><br>\n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-Image-Classification/blob/main/Pertemuan%205/5.2%20going_deeper_with_resnet_model.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><br><br><br>\n",
    "- Click `Connect` button in top right Google Colab notebook,<br>\n",
    "<img src=\"resource/cl-connect-gpu.png\" width=\"250px\">\n",
    "- If connecting process completed, it will turn to something look like this<br>\n",
    "<img src=\"resource/cl-connect-gpu-success.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check GPU connected into Colab environment is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Deeper Network vs Model Performance\n",
    "- In the past experiment, we are able to achieve a good enough performance for image classification by just simply stacking several layer into the network, <br>\n",
    "<img src=\"resource/small-net.png\" width=\"100%\"> <br>\n",
    "- What if we keep adding extra layer and making the network got bigger and bigger?\n",
    "    - Deeper convolutional neural networks some times beneficial to give a model ability to learn better.\n",
    "- Until the train and validation accuracy is <font color=\"orange\">saturated (won't changed)</font>, or even worst.<br>\n",
    "<img src=\"resource/deep-network.png\" width=\"100%\"><br>\n",
    "- This problem related to the network <font color=\"orange\">degradation</font>.\n",
    "    - Adding extra layer has no bennefit and accuracy remain the same.\n",
    "    - The network experiencing <font color=\"orange\">vanishing/exploding gradients</font>, making it hard to <font color=\"orange\">converge</font> by optimizer. <br>\n",
    "    <img src=\"resource/vanish-exploding-grad.png\" width=\"500px\"><br>\n",
    "- This is explain why stacking more layers in deep neural network, not always making model learn better.\n",
    "- When adding extra layer on that kind of situation, will just <font color=\"orange\">learn to do nothing</font> and the result is <font color=\"orange\">unchanged</font>.\n",
    "    - The layer now act like an <font color=\"orange\">Identity Function</font>, <br>\n",
    "    <img src=\"resource/Identity-Function.png\" width=\"550px\"><br>\n",
    "- On above illustration, we can verify that <font color=\"orange\">with or without the additional layer</font>, the result is <font color=\"orange\">unchanged</font>, \n",
    "    - So it makes sense if we just <font color=\"orange\">skip it</font> (a.k.a <font color=\"cyan\">Skip Connection</font>).    \n",
    "    - This ensures the network <font color=\"orange\">won’t degrade</font>.\n",
    "- <font color=\"cyan\">Skip Connection</font> allow the input to <font color=\"orange\">skip</font> a layer and get <font color=\"orange\">added</font> to the output of the next layer. \n",
    "    - This effectively means the network learns both the <font color=\"orange\">transformation</font> and an <font color=\"orange\">identity mapping</font>.<br>\n",
    "    <img src=\"resource/residual-block-cat.png\" width=\"600px\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Residual Block\n",
    "- Based on above idea Kaiming He et al in his paper *['Deep Residual Learning for Image Recognition' - arxiv.org](https://arxiv.org/abs/1512.03385)*, proposing <font color=\"orange\">Residual Block</font> to handling degradation problem in a very deep neural network. <br>\n",
    "<img src=\"resource/residual-block.png\" width=\"500px\"><br><i>regular block (left) vs Residual Block (right) - source [[link](https://d2l.ai/chapter_convolutional-modern/resnet.html)]</i><br><br>\n",
    "- It's called <font color=\"orange\">\"residual\"</font> because it represents the difference between the <font color=\"orange\">original signal</font> (input : $x$) and the <font color=\"orange\">modified signal</font> (output : $F(x)$).\n",
    "    - In the context of neural networks, a residual image captures what <font color=\"orange\">remains after subtracting</font> the modified from the original signal ($G(x)$). \n",
    "    - It’s like a <font color=\"orange\">visual residue</font> of the changes made. \n",
    "    - This concept helps models focus on learning changes or details that improve performance, rather than relearning everything from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementation of <font color=\"orange\">Residual Block</font> in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define <font color=\"orange\">Residual Block</font> following this structure, <br>\n",
    "<img src=\"resource/residual-block-2.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # Define first convolutional layer with batch normalization\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # Define second convolutional layer with batch normalization\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution -> batch normalization -> ReLU activation\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Apply second convolution -> batch normalization\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # Add the input to the output\n",
    "        out += x\n",
    "        # Apply final ReLU activation\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess an image\n",
    "image = cv2.imread(\"cat.jpg\")  # Load image 'cat.jpg' using OpenCV\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "# Convert image to tensor\n",
    "image = torch.from_numpy(image).to(torch.float32) # Convert Numpy array to PyTorch tensor\n",
    "image = image.permute(2, 0, 1)  # Change the order of dimensions from (H, W, C) to (C, H, W)\n",
    "input_image = image.unsqueeze(0)  # Add batch dimension (1, C, H, W)\n",
    "input_image = input_image / 255.0 # Normalize tensor to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a residual block and process the image\n",
    "rb = ResidualBlock(channels=3)\n",
    "residual_image = rb(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the residual (absolute difference between input and output)\n",
    "visual_residual = torch.abs(input_image - residual_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rescale_tensor(tensor, ):\n",
    "    # find min max value on current tensor\n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    \n",
    "    # Normalize the tensor to range [0, 1]\n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Scale to the new range to 0 - 255\n",
    "    rescaled_tensor = normalized_tensor * 255\n",
    "\n",
    "    # convert tensort to numpy array 8bit\n",
    "    rescaled_tensor = rescaled_tensor.squeeze().permute(1, 2, 0).detach().numpy().astype(np.uint8)\n",
    "    return rescaled_tensor\n",
    "\n",
    "# convert & rescale tensort to numpy array 8bit\n",
    "input_image_np = rescale_tensor(input_image)\n",
    "residual_image_np = rescale_tensor(residual_image)\n",
    "visual_residual_np = rescale_tensor(visual_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(input_image_np, residual_image_np, visual_residual_np) :\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(input_image_np)\n",
    "    ax[0].set_title('Input Image')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].imshow(residual_image_np)\n",
    "    ax[1].set_title('Output Image (After Residual Block)')\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    ax[2].imshow(visual_residual_np)\n",
    "    ax[2].set_title('Visual Residual')\n",
    "    ax[2].axis('off')\n",
    "\n",
    "\n",
    "# Plot the images\n",
    "imshow(input_image_np, residual_image_np, visual_residual_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Residual Block vs Dimension Missmatch\n",
    "- On above example, we discover how to implement Residual Block in Pytorch with some limitation,\n",
    "    1. <font color=\"orange\">Number of channel</font> in input and output must be <font color=\"orange\">exactly same</font>,\n",
    "        ```\n",
    "        rb = ResidualBlock(channels=3)\n",
    "        ```\n",
    "    2. <font color=\"orange\">Dimension size (W, H)</font> in input and output must be <font color=\"orange\">exactly same</font>, by setting convolution stride and padding to 1,\n",
    "        ```\n",
    "        nn.Conv2d(.... stride=1, padding=1 ....)\n",
    "        ```\n",
    "- Usually we <font color=\"orange\">reduce</font> dimension size (W, H) and increase number of output channel gradually in neural network.\n",
    "- This is requred by the network to learn <font color=\"orange\">more feature</font> and <font color=\"orange\">simplify abstaction</font> from the input into the output of the network.\n",
    "- With above implementation, we can't simply change the number of channel or reduce dimention, since that will make the network experiencing <font color=\"cyan\">dimension missmatch</font> when applying skip connection inside <font color=\"orange\">Residual Block</font>.<br>\n",
    "<img src=\"resource/dimention-missmatch.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To handling this problem, we can apply <font color=\"orange\">1x1 Convolution</font> before applying skip connection on input layer with <font color=\"orange\">parameterize</font> number of output <font color=\"orange\">channel</font> and <font color=\"orange\">stride</font>.\n",
    "- <font color=\"orange\">Residual block</font> with vs without <font color=\"orange\">1x1 Convolution</font>, which transforms the input into the desired shape for the addition operation.<br>\n",
    "<img src=\"resource/residual-block-3.png\" width=\"700px\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modified <font color=\"orange\">ResidualBlock()</font> implementation with additional 1x1 Convolution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # Define first convolutional layer with batch normalization\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Define second convolutional layer with batch normalization\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection to match input and output dimensions if needed\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels: # check dimension reduced (stride > 1) or number of channel change\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False), # apply 1x1 convolution\n",
    "                nn.BatchNorm2d(out_channels) # additional normalization\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution and batch normalization, then ReLU activation\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Apply second convolution and batch normalization\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # Add the shortcut connection to the output\n",
    "        out += self.shortcut(x)\n",
    "        # Apply final ReLU activation\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a residual block and process the image\n",
    "# double the number of channel with half size dimension\n",
    "\n",
    "rb = ResidualBlock(in_channels=3, out_channels=6, stride=2)\n",
    "residual_image = rb(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the output shape\n",
    "\n",
    "residual_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 ResNet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On above example we already cover how to implement Residual block even if the dimantion size (W,H) and/or number of channel (C) is missmatch.\n",
    "- Now we will learn how Kaiming He et al implementing Residual Block into ResNet model architecture.\n",
    "- There is five version ResNet model which contain 18, 34, 50, 101, 152 layers.\n",
    "- Here now we will discover the simple one, ResNet-18 which stacked by 18 layers. \n",
    "<img src=\"resource/Resnet-18-Model.png\" width=\"100%\"><br><i>simplified ResNet18 architecture - source [[link](https://d2l.ai/chapter_convolutional-modern/resnet.html)]</i><br><br><br>\n",
    "- Full structured ResNet-18 Architecture\n",
    "<img src=\"resource/Resnet-18-Model-Long.png\" width=\"100%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementation ResNet-18 in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Residual Block\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Define first convolutional layer with batch normalization\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Define second convolutional layer with batch normalization\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection to match input and output dimensions if needed\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels: # check dimension reduced (stride > 1) or number of channel change\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False), # apply 1x1 convolution\n",
    "                nn.BatchNorm2d(out_channels) # additional normalization\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution and batch normalization, then ReLU activation\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Apply second convolution and batch normalization\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # Add the shortcut connection to the output\n",
    "        out += self.shortcut(x)\n",
    "        # Apply final ReLU activation\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.in_channels = 64 # initial input channel size\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Layer definitions\n",
    "        self.layer1 = self._make_layer(block, num_blocks[0], out_channels=64, stride=1)\n",
    "        self.layer2 = self._make_layer(block, num_blocks[1], out_channels=128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, num_blocks[2], out_channels=256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, num_blocks[3], out_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    # Function to create layers\n",
    "    def _make_layer(self, block, num_block, out_channels, stride):\n",
    "        \n",
    "        # first block has specified stride, others have stride 1\n",
    "        # stride = 1, num_block = 2 then will resulting stride_list = [1, 1]\n",
    "        # stride = 2, num_block = 2 then will resulting stride_list = [2, 1]\n",
    "        stride_list = [stride] + [1] * (num_block - 1)  \n",
    "\n",
    "        layers = []\n",
    "        for stride in stride_list: \n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels  # ppdate input channel size for next block\n",
    "\n",
    "        # *layers: The * operator unpacks the layers list, \n",
    "        # so each layer/block is treated as an individual argument to nn.Sequential.\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instantiate the ResNet-18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes=1000):\n",
    "    \n",
    "    # ResNet-18 stacked by 4 x 2 residual block.\n",
    "    # with dimentionality reduction in the 2nd, 3rd and 4th residual block by setting stride = 2. \n",
    "    return ResNet(ResidualBlock, [2, 2, 2, 2], num_classes=num_classes) \n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNet18(num_classes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy input tensor\n",
    "input_tensor = torch.randn(1, 3, 224, 224)  # Batch size of 1, 3 color channels, 224x224 image\n",
    "\n",
    "# Pass the dummy input through the model\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output.shape)  # Should be (1, 1000) for ImageNet classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 Train ResNet-18 using MNIST Digit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import os\n",
    "import gdown\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# clear output cell\n",
    "display.clear_output()\n",
    "\n",
    "print(f\"torch : {torch.__version__}\")\n",
    "print(f\"torch vision : {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'MNIST' # the dataset name\n",
    "DATASET_NUM_CLASS = 10 # number of class in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default using gdrive_id Dataset `mnist_dataset.zip` (1-FfwJrllyHofQwIbMb_IxAkxnfMGSFmR)\n",
    "gdrive_id = '1-FfwJrllyHofQwIbMb_IxAkxnfMGSFmR' # <-----  ⚠️⚠️⚠️ USE YOUR OWN GDrive ID FOR CUSTOM DATASET ⚠️⚠️⚠️\n",
    "\n",
    "# download zip from GDrive\n",
    "url = f'https://drive.google.com/uc?id={gdrive_id}'\n",
    "gdown.download(url, DATASET_NAME + \".zip\", quiet=False)\n",
    "\n",
    "# unzip dataset\n",
    "!unzip {DATASET_NAME}.zip -d {DATASET_NAME}\n",
    "\n",
    "# clear output cell\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Dataset class\n",
    "# it's just helper to load image dataset using OpenCV and convert to pytorch tensor\n",
    "# also doing a label encoding using one-hot encoding\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = sorted([file for file in os.listdir(root_dir) if file.lower().endswith('.png')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read image from corresponding .png file\n",
    "        image_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = cv2.imread(image_path)  # Load image using OpenCV\n",
    "        image = cv2.resize(image, (224,224)) # resize to make it comply with ResNet input size\n",
    "        image = torch.from_numpy(image).to(torch.float32)  # Convert NumPy array to PyTorch tensor\n",
    "        image = image.permute(2, 0, 1)  # Change the order of dimensions from (H, W, C) to (C, H, W)\n",
    "\n",
    "        # Read label from corresponding .txt file\n",
    "        label_path = os.path.splitext(image_path)[0] + \".txt\"\n",
    "        with open(label_path, 'r') as label_file:\n",
    "            label = int(label_file.read().strip())  # Assuming labels are integers\n",
    "\n",
    "        # Apply one-hot encoding into label\n",
    "        labels_tensor = torch.tensor(label)\n",
    "        one_hot_encoded = F.one_hot(labels_tensor, num_classes=DATASET_NUM_CLASS).to(torch.float32)\n",
    "\n",
    "        return image, one_hot_encoded\n",
    "\n",
    "\n",
    "\n",
    "# instantiate dataset\n",
    "# in here the image dataset is not loaded yet\n",
    "# we only read all image files names in fataset folder\n",
    "all_train_dataset = CustomDataset(root_dir=f'{DATASET_NAME}/dataset/train')\n",
    "test_dataset = CustomDataset(root_dir=f'{DATASET_NAME}/dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"All Train Dataset : {len(all_train_dataset)} data\")\n",
    "print(f\"Test Dataset : {len(test_dataset)} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'all_train_dataset' into 'train' and 'validation' set using `random_split()` function\n",
    "train_dataset, validation_dataset = random_split(all_train_dataset, [50000, 10000])\n",
    "\n",
    "print(f\"Train Dataset : {len(train_dataset)} data\")\n",
    "print(f\"Validation Dataset : {len(validation_dataset)} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instantiate <font color=\"orange\">ResNet-18</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# instantiate ResNet-18 with MNIST Dataset of 10 class data\n",
    "model = ResNet18(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer, loss function & metric\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To run training process, we can use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Add progress bar for training loop\n",
    "    progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs = inputs.to(device) # move inputs to device\n",
    "        labels = labels.to(device) # move labels to device\n",
    "\n",
    "        # resets the gradients of all the model's parameters before the backward pass\n",
    "        optimizer.zero_grad()\n",
    "        # pass 2D 28x28 input tensor to CNN model\n",
    "        outputs = model(inputs)\n",
    "        # calc loss value\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # computes the gradient of the loss with respect to each parameter in model\n",
    "        loss.backward()\n",
    "        # adjust model parameter\n",
    "        optimizer.step()\n",
    "        # sum loss value\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate correct & total prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels.argmax(1)).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Update progress bar description with current loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate average training loss\n",
    "    average_train_loss = running_loss / len(train_loader.dataset)\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    return average_train_loss, train_accuracy\n",
    "\n",
    "def validate(model, val_loader, loss_function):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Add progress bar for validation loop\n",
    "    progress_bar = tqdm(val_loader, desc='Validating', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device) # move inputs to device\n",
    "            labels = labels.to(device) # move labels to device\n",
    "\n",
    "            # pass 2D 28x28 input tensor to CNN model\n",
    "            outputs = model(inputs)\n",
    "            # calc loss value\n",
    "            loss = loss_function(outputs, labels)\n",
    "            # sum loss value\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate correct & total prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels.argmax(1)).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            # Update progress bar description with loss\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    average_val_loss = running_loss / len(val_loader.dataset)\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "    return average_val_loss, val_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This is a training loop for selected Epoch\n",
    "# each epoch will process all training and validation set, chunked into small batch size data\n",
    "# then measure the loss & accuracy of training and validation set\n",
    "NUM_EPOCH = 10      # you can change this value\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCH}\")\n",
    "\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, loss_function)\n",
    "    val_loss, val_accuracy = validate(model, validation_loader, loss_function)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy * 100)  # convert to percentage\n",
    "    val_accuracies.append(val_accuracy * 100)  # convert to percentage\n",
    "\n",
    "    print(f\"Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot Loss and Accuracy of Training vs Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Loss & Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, NUM_EPOCH + 1))\n",
    "\n",
    "# Plotting loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate Model, find Precision, Recal each class data, measure accuracy and compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# define evaluate function for test set\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Add progress bar for validation loop\n",
    "    progress_bar = tqdm(test_loader, desc='Evaluating', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # iterate over all batched test set\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device) # move inputs to device\n",
    "            labels = labels.to(device) # move labels to device\n",
    "\n",
    "            # pass 2D 28x28 input tensor to CNN model\n",
    "            outputs = model(inputs)\n",
    "            # get prediction\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # collect all labels & preds\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Evaluation on test set\n",
    "all_labels, all_preds = evaluate(model, test_loader)\n",
    "all_labels = np.argmax(all_labels, axis=1)\n",
    "\n",
    "# Calculate classification report\n",
    "labels = [str(i) for i in range(DATASET_NUM_CLASS)]\n",
    "print(classification_report(all_labels, all_preds, target_names=labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'trained_cnn_model.pt')\n",
    "\n",
    "# Download the model file\n",
    "from google.colab import files\n",
    "files.download('trained_cnn_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 State of The Art Image Classification Model\n",
    "- Historical timeline of key <font color=\"orange\">deep learning models</font> and innovations in <font color=\"orange\">image classification</font>. \n",
    "- Highlighting major breakthroughs and architectural advancements: <br><br>\n",
    "<img src=\"resource/ic-sota.png\" width=\"95%\"><br>\n",
    "    - üèÜ : [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://www.image-net.org/challenges/LSVRC/) Winner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Historical <font color=\"orange\">innovations</font> in deep learning for <font color=\"orange\">image classification</font>, including new types of <font color=\"orange\">layers</font>, <font color=\"orange\">techniques</font>, and other key <font color=\"orange\">advancements</font>.\n",
    "- Highlighting major breakthroughs and architectural advancements:<br><br>\n",
    "<img src=\"resource/innovation-sota.png\" width=\"95%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- <font color=\"orange\">AlexNet</font> : \n",
    "    - Author : Alex Krizhevsky et al (University of Toronto)  \n",
    "    - Architecture : 8 layers (5 convolutional, 3 fully connected)<br>\n",
    "        <img src=\"resource/AlexNet.png\" width=\"600px\"><br><i>AlexNet - source [[link](https://www.researchgate.net/figure/Architecture-of-AlexNet_fig1_344317236)]</i><br>\n",
    "    - Innovation : \n",
    "        - ReLU Activation : Speeds up training.\n",
    "        - Dropout : Prevents overfitting.\n",
    "    - Paper : *['One weird trick for parallelizing convolutional neural networks' - arxiv.org\n",
    "](https://arxiv.org/abs/1404.5997)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"orange\">GoogLeNet (Inception v1)</font> :\n",
    "    - Author : Christian Szegedy et al (<img src=\"https://www.google.com/favicon.ico\" width=\"12px\"> Google Brain)\n",
    "    - Architecture : 22 layers deep with an <font color=\"orange\">Inception Module</font><br>\n",
    "        <img src=\"resource/google-net-like.png\" width=\"800px\"><br><i>GoogLeNet - source [[link](https://viso.ai/deep-learning/googlenet-explained-the-inception-model-that-won-imagenet/)]</i><br>\n",
    "    - Innovation :\n",
    "        - <font color=\"orange\">Inception Module</font> : Uses multiple filter sizes in parallel to capture fine to coarse details.\n",
    "    - Paper : *['Going Deeper with Convolutions' - arxiv.org](https://arxiv.org/abs/1409.4842)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"orange\">ResNet (Residual Networks)</font> :\n",
    "    - Author : Kaiming He et al (<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Microsoft_icon.svg/240px-Microsoft_icon.svg.png\" width=\"12px\"> Microsoft Research)\n",
    "    - Architecture : Deep networks using <font color=\"orange\">Residual Blocks</font>.<br>\n",
    "        <img src=\"resource/Resnet-18-Model.png\" width=\"900px\"><br><i>ResNet18 - source [[link](https://d2l.ai/chapter_convolutional-modern/resnet.html)]</i><br><br>\n",
    "        <img src=\"resource/residual-block.png\" width=\"500px\"><br><i>regular block (left) vs Residual Block (right) - source [[link](https://d2l.ai/chapter_convolutional-modern/resnet.html)]</i><br>\n",
    "\n",
    "    - Innovation :\n",
    "        - Residual Connections (<font color=\"orange\">Skip Connections</font>) : Bypass certain layers, allowing the network to train very deep models.\n",
    "    - Paper : *['Deep Residual Learning for Image Recognition' - arxiv.org](https://arxiv.org/abs/1512.03385)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"orange\">ResNeXt</font> : \n",
    "    - Author : Saining Xie et al (<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Microsoft_icon.svg/240px-Microsoft_icon.svg.png\" width=\"12px\"> Microsoft Research)\n",
    "    - Architecture : Builds on ResNet with a modular structure using <font color=\"orange\">Grouped Convolutions</font>.<br>\n",
    "        <img src=\"resource/ReNeXt-block.png\" width=\"700px\"><br><i>ResNeXt block - source [[link](https://d2l.ai/chapter_convolutional-modern/resnet.html)]</i><br>\n",
    "    - Innovation : \n",
    "        - <font color=\"orange\">Grouped Convolutions</font> : Efficiently expands the model‚Äôs capacity by dividing the convolutions into smaller groups.\n",
    "    - Paper : *['Aggregated Residual Transformations for Deep Neural Networks' - arxiv.org](https://arxiv.org/abs/1611.05431)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"orange\">SENet (Squeeze-and-Excitation Networks)</font> :\n",
    "    - Author : Jie Hu et al (Institute of Software,Chinese Academy of Sciences)\n",
    "    - Architecture : Adds a <font color=\"orange\">Squeeze-and-Excitation (SE) Block</font> to existing architectures like ResNet.<br>\n",
    "        <img src=\"resource/se-block.png\" width=\"800px\"><br><i>Squeeze-and-Excitation block - source [[link](https://arxiv.org/pdf/1709.01507v4)]</i><br>\n",
    "    - Innovation : \n",
    "        - <font color=\"orange\">Squeeze-and-Excitation Block</font> : It adaptively recalibrates the importance of each feature channel.\n",
    "    - Paper : *['Squeeze-and-Excitation Networks' - arxiv.org](https://arxiv.org/abs/1709.01507)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"orange\">EfficientNet</font> :\n",
    "    - Author : Mingxing Tan et al (<img src=\"https://www.google.com/favicon.ico\" width=\"12px\"> Google Brain)\n",
    "    - Architecture : Efficient scaling method using <font color=\"orange\">Compound Scaling</font> to balance depth, width, and resolution of the network.<br>\n",
    "        <img src=\"resource/compound-scalling.png\" width=\"800px\"><br><i>Compound Scalling</font> - source [[link](https://arxiv.org/pdf/1905.11946)]</i><br>\n",
    "    - Innovation : \n",
    "        - <font color=\"orange\">Compound Scaling</font> : Instead of scaling these dimensions independently, EfficientNet scales them uniformly, improving efficiency.\n",
    "    - Paper : *['EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks' - arxiv.org](https://arxiv.org/abs/1905.11946)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"orange\">Vision Transformer (ViT)</font> :\n",
    "    - Author : Alexey Dosovitskiy et al (<img src=\"https://www.google.com/favicon.ico\" width=\"12px\"> Google Brain)\n",
    "    - Architecture : Uses the <font color=\"orange\">Transformer</font> architecture utilizing <font color=\"orange\">Multi-head Attention</font>, originally designed for NLP tasks, to process images.<br>\n",
    "        <img src=\"resource/vision-transformer.png\" width=\"800px\"><br><i>Vision Transformer</font> - source [[link](https://arxiv.org/pdf/1905.11946)]</i><br>\n",
    "    - Innovation : \n",
    "        - <font color=\"orange\">Patch Embedding</font> : Splits images into fixed-size patches (like tokens in NLP) and processes them as sequences using Transformer layers.\n",
    "    - Paper : *['An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale' - arxiv.org](https://arxiv.org/abs/2010.11929)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"orange\">Swin Transformer</font> :\n",
    "    - Author : Ze Liu et al (<img src=\"data:image/webp;base64,UklGRoACAABXRUJQVlA4IHQCAABQDwCdASo4ADgAPjEQjEYiEREJACADBLSACIO/iXkuXsvzAahv+p/k5+R2ZA/uv5Y6w7+Wf43Us/7T0Z/3z7lfan80/673Av43/R/8J+YX7//SB62v2j9jD9Y1MozIuFg+0+KbPwt6gDyvaTkWcN/gdgzMjR9PBumwgWFkYJwTgmYvwAD+/vho3/4RmeJw9d5sRHzhWRvGiQ80GHlEoPTepbVvmz3N9+bfn//rrmZUdEIDC0OICp0ZDm+44hvu57VSQcPztCqz4dpOtak0zB4M+ASxPyTq2l1YjYeGkkBuFKCbWg3OACllTDvXfHtcB/e/unf//6T5b6/YePEHYvuAOXXH//kF4CYGpW6AkofUh636w5v8onHeLFEFP5GgSX6LtRni2K6ZHY1bQvqyM8ii70/PoU8JdHu3FYBFJiXEAGAdxnAJQkePm8JxM6f6yrFo8rzKoWlEWmPr2vKRLamzhfvKXghGq9ltkyf/s+uKPBs9lYysuU1vhnI9M9q6k7ABj4pg+uR80U/3knGNPuns508cOOjN/noMAIrvIon9M9lfCUY/+TPIx/Be8ZSMl0IPn+Nw8SOSNNT23LC2VTEi/9ULf8bhZR2ZzuoB9r0l7lZY/eX6NUV1AYv/3zX/E1bhWv5EZj5eg53Yz2l8NVeur7WX5+rV8wmcq0zteho8PrdCo0jiBqU9upsvmK16+VdYuUqcyXdO11YXAnITTnMptlGojB0Ycg2RH//TnBYKpJsHmhwJNiVaCrb7bgbp7f5Au0MjlK+nxpqFeWEgqLZP/b7B6mPCn4/ZiH/dpzdBie8YKpm5TOI1EdkmY1QzRBhAAAAA\" width=\"14px\"> xAI)\n",
    "    - Architecture : A hierarchical Transformer designed for vision tasks, using <font color=\"orange\">Shifted Windows Mechanism</font> to process image patches.<br>\n",
    "        <img src=\"resource/swin-transformer.png\" width=\"500px\"><br><i>Swin Transformer compute attention in local window (red block) - source [[link](https://arxiv.org/pdf/1905.11946)]</i><br><br>\n",
    "        <img src=\"resource/swin-transformer-2.png\" height=\"250px\"><img src=\"resource/swin.png\" height=\"250px\"><br><i>two successive Swin Transformer Blocks using *shifted window* approach for computing self-attention - source [[link](https://arxiv.org/pdf/1905.11946)]</i><br>\n",
    "    - Innovation : \n",
    "        - <font color=\"orange\">Shifted Window Mechanism (SWM)</font> : Processes images by dividing them into non-overlapping windows and shifting the windows between layers, allowing local and global interactions efficiently.\n",
    "    - Paper : *['Swin Transformer: Hierarchical Vision Transformer using Shifted Windows' - arxiv.org](https://arxiv.org/abs/2103.14030)*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

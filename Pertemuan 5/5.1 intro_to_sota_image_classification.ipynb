{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 State of The Art Image Classification Model\n",
    "- Historical timeline of key <font color=\"orange\">deep learning models</font> and innovations in <font color=\"orange\">image classification</font>. \n",
    "- Highlighting major breakthroughs and architectural advancements: <br><br>\n",
    "<img src=\"resource/ic-sota.png\" width=\"95%\"><br><br>\n",
    "    - üèÜ : [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://www.image-net.org/challenges/LSVRC/) Winner.\n",
    "    - AlexNet : \n",
    "        - Author : Alex Krizhevsky et al (University of Toronto)  \n",
    "        - Architecture : 8 layers (5 convolutional, 3 fully connected)\n",
    "        - Innovation : \n",
    "            - ReLU Activation: Speeds up training.\n",
    "            - Dropout: Prevents overfitting.\n",
    "        - Paper : *['One weird trick for parallelizing convolutional neural networks' - arxiv.org\n",
    "](https://arxiv.org/abs/1404.5997)*\n",
    "    - GoogLeNet (Inception v1) :\n",
    "        - Author : Christian Szegedy et al (Google Research)\n",
    "        - Architecture : 22 layers deep with an Inception module\n",
    "        - Innovation :\n",
    "            - Inception Module : Uses multiple filter sizes in parallel to capture fine to coarse details\n",
    "            - Global Average Pooling : Replaces fully connected layers, reducing overfitting\n",
    "        - Paper : *['Going Deeper with Convolutions' - arxiv.org](https://arxiv.org/abs/1409.4842)*\n",
    "    - ResNet (Residual Networks) :\n",
    "        - Author : Kaiming He et al (Microsoft Research)\n",
    "        - Architecture : Deep networks using residual blocks.\n",
    "        - Innovation :\n",
    "            - Residual Connections (Skip Connections): Bypass certain layers, allowing the network to train very deep models.\n",
    "        - Paper : *['Deep Residual Learning for Image Recognition' - arxiv.org](https://arxiv.org/abs/1512.03385)*\n",
    "    - ResNeXt : \n",
    "        - Author : Saining Xie et al (Microsoft Research)\n",
    "        - Architecture : Builds on ResNet with a modular structure using grouped convolutions.\n",
    "        - Innovation : \n",
    "            - Grouped Convolutions: Efficiently expands the model‚Äôs capacity by dividing the convolutions into smaller groups.\n",
    "        - Paper : *['Aggregated Residual Transformations for Deep Neural Networks' - arxiv.org](https://arxiv.org/abs/1611.05431)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Historical <font color=\"orange\">innovations</font> in deep learning for <font color=\"orange\">image classification</font>, including new types of <font color=\"orange\">layers</font>, <font color=\"orange\">techniques</font>, and other key <font color=\"orange\">advancements</font>.\n",
    "- Highlighting major breakthroughs and architectural advancements:<br><br>\n",
    "<img src=\"resource/innovation-sota.png\" width=\"95%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Image classification on [ImageNet](https://www.image-net.org/), benchmark on [paperwithcode.com](https://paperswithcode.com/sota/image-classification-on-imagenet),\n",
    "<img src=\"resource/model-sota.png\" width=\"95%\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

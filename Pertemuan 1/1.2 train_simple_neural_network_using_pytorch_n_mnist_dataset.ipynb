{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Train Simple Neural Network using Pytorch on MNIST Dataset\n",
    "- About MNIST Dataset\n",
    "- Load and Split Dataset Split\n",
    "- Label Encoding\n",
    "- Create Simple Neural Network Model\n",
    "- Run Training Model\n",
    "- Visualize Training Loss vs Trining Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 About MNIST Dataset\n",
    "- We will use the **MNIST dataset**, a collection of **60,000** labeled handwritten digits dataset in 10 classes.<br>\n",
    "- Handwritten digits in the MNIST dataset are <i><font color='orange'>28x28 pixel</font></i> grayscale images. \n",
    "- The neural network we will build <i><font color='orange'>classifies the handwritten digits</font></i> in their **10 classes** (0, .., 9).\n",
    "<img src=\"resource/MNIST.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Load & Split Dataset \n",
    "- Load each image on MNIST dataset using OpenCV\n",
    "- <i><font color='orange'>Convert to gray</font></i>, to make sure we only have single channel of 28x28 pixel data on each image\n",
    "- The simplest approach for classifying them is to use the <i><font color='orange'>28x28=784 pixels</font></i> as inputs for a 1-layer neural network.\n",
    "- That why we will convert the <i><font color='orange'>2D matrix of 28x28 pixel</font></i> into flatten <i><font color='orange'>1D array 784 pixel</font></i>.<br>\n",
    "<img src=\"resource/MNIST_Load.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural Network learned through a training process which requires a <i><font color='orange'>\"training dataset\"</font></i>. \n",
    "- We <i><font color='cyan'>need another dataset, never seen during training</font></i>, to evaluate the \"real-world\" performance of the network. It is called an <i><font color='orange'>\"validation dataset\"</font></i>.\n",
    "- Here we split the 60.000 labeled images MNIST dataset into <i><font color='orange'>60.000 data</font></i> for <i><font color='orange'>\"training dataset\"</font></i> and <i><font color='orange'>10.000 data</font></i> for <i><font color='orange'>\"validation dataset\"</font></i> <br>\n",
    "<img src=\"resource/MNIST_split.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Label Encoding\n",
    "- Label Encoding help model to map label as <i><font color='orange'>numerical representation</font></i> with ordering.\n",
    "- Model might learn some <i><font color='orange'>natural ordering between the different class labels</font></i> based on the labels. \n",
    "- Assigning them <i><font color='orange'>numbers in a scale</font></i> would implicitly create ordering and relations between different classes.\n",
    "- For this purpose, we will use <i><font color='orange'>One-hot Encoding</font></i> to encode label of MNIST dataset into look like this,<br>\n",
    "<img src=\"resource/MNIS_OneHot.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code apply one hot encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Create Simple Neural Network Model\n",
    "- We will creating a simple Neural Network model, with only Input and Output Layer\n",
    "- The <i><font color='cyan'>Input Layer</font></i> will have <i><font color='orange'>784</font></i> neuron (*same size with flattened 28x28=784 pixels on each MNIST dataset*)\n",
    "- The <i><font color='cyan'>Output Layer</font></i> will have <i><font color='orange'>10 neuron</font></i> (*same size with a number of dataset class*)<br>\n",
    "<img src=\"resource/NN_SingleDense.png\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each \"neuron\" in a neural network does a <i><font color='cyan'>weighted sum of all of its inputs</font></i>, adds a constant called the <i><font color='cyan'>\"bias\"</font></i>.\n",
    "<img src=\"resource/NN_03.png\" width=\"400px\"><br><br>\n",
    "- On above network will have <i><font color='cyan'>784x10 weight</font></i>.<br>\n",
    "<img src=\"resource/NN_WeightedSum.gif\" width=\"600px\"><br><br>\n",
    "- Then feeds the result through some non-linear <i><font color='cyan'>\"activation function\"</font></i>. <br>\n",
    "- We will use <i><font color='cyan'>Softmax</font></i> for that purpose.<br>\n",
    "<img src=\"resource/NN_08.png\" height=\"175px\"><img src=\"resource/NN_07.gif\" height=\"175px\"><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then to <i><font color='cyan'>measure how good</font></i> the trained model, we will measure distance between what the <i><font color='cyan'>network tells us</font></i> and the <i><font color='cyan'>correct answers</font></i>.\n",
    "- For classification problems we will use <i><font color='cyan'>\"cross-entropy distance\"</font></i> (a.k.a loss function).\n",
    "<img src=\"resource/NN_LOSS.png\" width=\"600px\"><br><br>\n",
    "- <i><font color='cyan'>\"Training\"</font></i> model actually means using training images and labels to <i><font color='cyan'>adjust weights</font></i> and <i><font color='cyan'>biases</font></i> so as to <i><font color='cyan'>minimise</font></i> the <i><font color='cyan'>cross-entropy</font></i> loss function.\n",
    "- That process is called <i><font color='cyan'>Optimizer</font></i>. We will talk more about this in <i><font color='cyan'>Pertemuan 2</font></i>, but keep in mind we will use <i><font color='orange'>SGD Optimizer</font></i> for now.<br><br>\n",
    "- We are also able to add metric to measure like model accuracy, precission, recall, or etc.\n",
    "- For now we just use <i><font color='orange'>accuracy metric</font></i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code setup optimizer, loss function & metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Run Training Model\n",
    "- To run training process, we can use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Visualize Loss vs Accuracy\n",
    "- We are also able to visualize Loss vs Accuracy using the following code,\n",
    "- <i><font color='orange'>Loss will decrease</font></i> as the <i><font color='orange'>epoch increases</font></i>, whereas <i><font color='orange'>accuracy will increase</font></i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code visualize Loss & Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "# Source \n",
    "- https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist#2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Experiment with Bigger Neural Network \n",
    "- Adding Layer \n",
    "- Change Activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è *Please open this notebook in Google Colab* by click below link ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è<br><br>\n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-Image-Classification/blob/main/Pertemuan%202/2.1%20bigger_network.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><br><br><br>\n",
    "- Click `Connect` button in top right Google Colab notebook,<br>\n",
    "<img src=\"resource/cl-connect-gpu.png\" width=\"250px\">\n",
    "- If connecting process completed, it will turn to something look like this<br>\n",
    "<img src=\"resource/cl-connect-gpu-success.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check GPU connected into Colab environment is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All code remain same with <font color=\"orange\">Pertemuan 1</font>, but different in designing model part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import gdown\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# clear output cell\n",
    "display.clear_output()\n",
    "\n",
    "print(f\"torch : {torch.__version__}\")\n",
    "print(f\"torch vision : {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'MNIST' # the dataset name\n",
    "DATASET_NUM_CLASS = 10 # number of class in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default using gdrive_id Dataset `mnist_dataset.zip` (1-FfwJrllyHofQwIbMb_IxAkxnfMGSFmR)\n",
    "gdrive_id = '1-FfwJrllyHofQwIbMb_IxAkxnfMGSFmR' # <-----  ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è USE YOUR OWN GDrive ID FOR CUSTOM DATASET ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "\n",
    "# download zip from GDrive\n",
    "url = f'https://drive.google.com/uc?id={gdrive_id}'\n",
    "gdown.download(url, DATASET_NAME + \".zip\", quiet=False)\n",
    "\n",
    "# unzip dataset\n",
    "!unzip {DATASET_NAME}.zip -d {DATASET_NAME}\n",
    "\n",
    "# clear output cell\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Dataset class\n",
    "# it's just helper to load image dataset using OpenCV and convert to pytorch tensor\n",
    "# also doing a label encoding using one-hot encoding\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = sorted([file for file in os.listdir(root_dir) if file.lower().endswith('.png')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read image from corresponding .png file\n",
    "        image_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = cv2.imread(image_path)  # Load image using OpenCV\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert BGR to GRAY\n",
    "        image = torch.from_numpy(image).to(torch.float32)  # Convert NumPy array to PyTorch tensor\n",
    "        image = torch.flatten(image) # flatten image from 2D Tensor (28x28) into a 1D tensor (784)\n",
    "\n",
    "        # Read label from corresponding .txt file\n",
    "        label_path = os.path.splitext(image_path)[0] + \".txt\"\n",
    "        with open(label_path, 'r') as label_file:\n",
    "            label = int(label_file.read().strip())  # Assuming labels are integers\n",
    "\n",
    "        # Apply one-hot encoding into label\n",
    "        labels_tensor = torch.tensor(label)\n",
    "        one_hot_encoded = F.one_hot(labels_tensor, num_classes=DATASET_NUM_CLASS).to(torch.float32)\n",
    "\n",
    "        return image, one_hot_encoded\n",
    "\n",
    "\n",
    "\n",
    "# instantiate dataset\n",
    "# in here the image dataset is not loaded yet\n",
    "# we only read all image files names in fataset folder\n",
    "all_train_dataset = CustomDataset(root_dir=f'{DATASET_NAME}/dataset/train')\n",
    "test_dataset = CustomDataset(root_dir=f'{DATASET_NAME}/dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"All Train Dataset : {len(all_train_dataset)} data\")\n",
    "print(f\"Test Dataset : {len(test_dataset)} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'all_train_dataset' into 'train' and 'validation' set using `random_split()` function\n",
    "train_dataset, validation_dataset = random_split(all_train_dataset, [50000, 10000])\n",
    "\n",
    "print(f\"Train Dataset : {len(train_dataset)} data\")\n",
    "print(f\"Validation Dataset : {len(validation_dataset)} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Experiment Adding more Layer to the Model\n",
    "- To improve the recognition accuracy we will <font color=\"orange\">add more layers</font> to the neural network.<br><br>\n",
    "<img src=\"resource/NN_arch_1.png\" width=\"700px\"><br><br>\n",
    "- We keep softmax as the activation function on the last layer because that is what works best for classification. \n",
    "- On <font color=\"orange\">hidden layers</font> however we will use the most classical activation function: the <font color=\"orange\">sigmoid</font>:<br>\n",
    "<img src=\"resource/NN_Sigmoid.png\" width=\"400px\"><a id=\"model-definition\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a modified model with additional layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=784, out_features=200),   # 1st hidden layer\n",
    "    nn.Sigmoid(),                                   # Sigmoid Activation function\n",
    "    nn.Linear(in_features=200, out_features=60),    # 2nd hidden layer\n",
    "    nn.Sigmoid(),                                   # Sigmoid Activation function\n",
    "    nn.Linear(in_features=60, out_features=10),     # Output layer\n",
    "    nn.LogSoftmax(dim=1)                            # Log probabilities for classification\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer, loss function & metric\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To run training process, we can use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Add progress bar for training loop\n",
    "    progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs = inputs.to(device) # move inputs to device\n",
    "        labels = labels.to(device) # move labels to device\n",
    "\n",
    "        # resets the gradients of all the model's parameters before the backward pass\n",
    "        optimizer.zero_grad()\n",
    "        # pass input tensor to model\n",
    "        outputs = model(inputs)\n",
    "        # calc loss value\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # computes the gradient of the loss with respect to each parameter in model\n",
    "        loss.backward()\n",
    "        # adjust model parameter\n",
    "        optimizer.step()\n",
    "        # sum loss value\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate correct & total prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels.argmax(1)).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Update progress bar description with current loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate average training loss\n",
    "    average_train_loss = running_loss / len(train_loader.dataset)\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    return average_train_loss, train_accuracy\n",
    "\n",
    "def validate(model, val_loader, loss_function):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Add progress bar for validation loop\n",
    "    progress_bar = tqdm(val_loader, desc='Validating', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device) # move inputs to device\n",
    "            labels = labels.to(device) # move labels to device\n",
    "\n",
    "            # pass input tensor to model\n",
    "            outputs = model(inputs)\n",
    "            # calc loss value\n",
    "            loss = loss_function(outputs, labels)\n",
    "            # sum loss value\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate correct & total prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels.argmax(1)).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            # Update progress bar description with loss\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    average_val_loss = running_loss / len(val_loader.dataset)\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "    return average_val_loss, val_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This is a training loop for selected Epoch\n",
    "# each epoch will process all training and validation set, chunked into small batch size data\n",
    "# then measure the loss & accuracy of training and validation set\n",
    "NUM_EPOCH = 10      # you can change this value\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCH}\")\n",
    "\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, loss_function)\n",
    "    val_loss, val_accuracy = validate(model, validation_loader, loss_function)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy * 100)  # convert to percentage\n",
    "    val_accuracies.append(val_accuracy * 100)  # convert to percentage\n",
    "\n",
    "    print(f\"Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot Loss and Accuracy of Training vs Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Loss & Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, NUM_EPOCH + 1))\n",
    "\n",
    "# Plotting loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate Model, find Precision, Recal each class data, measure accuracy and compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# define evaluate function for test set\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Add progress bar for validation loop\n",
    "    progress_bar = tqdm(test_loader, desc='Evaluating', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # iterate over all batched test set\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device) # move inputs to device\n",
    "            labels = labels.to(device) # move labels to device\n",
    "\n",
    "            # pass input tensor to model\n",
    "            outputs = model(inputs)\n",
    "            # get prediction\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # collect all labels & preds\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Evaluation on test set\n",
    "all_labels, all_preds = evaluate(model, test_loader)\n",
    "all_labels = np.argmax(all_labels, axis=1)\n",
    "\n",
    "# Calculate classification report\n",
    "labels = [str(i) for i in range(DATASET_NUM_CLASS)]\n",
    "print(classification_report(all_labels, all_preds, target_names=labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'trained_model.pt')\n",
    "\n",
    "# Download the model file\n",
    "from google.colab import files\n",
    "files.download('trained_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    ">## Discussion\n",
    ">- Is adding more layer can improve classification performance?\n",
    ">- Try changing Activation Function to <font color=\"cyan\">ReLU</font>. \n",
    ">   - Go to [model definition](#scrollTo=N8cHrdYFi1hL), try changing the hidden layer activation function from <font color=\"orange\">Sigmoid</font> into <font color=\"orange\">ReLU</font><br>\n",
    ">       <img src=\"resource/NN_05.png\" width=\"350px\"><br><br>\n",
    ">       - ReLU Activation Function in pytorch\n",
    ">            ```\n",
    ">            nn.ReLU()\n",
    ">            ```\n",
    ">       - Re-run the training process and compare the performance result\n",
    ">       - Is changing activation function give any impact?\n",
    ">\n",
    ">."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìùüìùüìù Special care for Deep Networks\n",
    "- As you add layers, neural networks have more and more <font color=\"orange\">difficulties to converge</font>.\n",
    "- But why the start-the-art algorithm like YOLO with a lot of layers in it, has a good performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. RELU activation\n",
    "- One of the answers is <font color=\"orange\">ReLU (Rectified Linear Unit)</font>,<br>\n",
    "<img src=\"resource/RELU.png\" width=\"500px\"><br><br>\n",
    "- The <font color=\"orange\">sigmoid activation function</font> is actually quite <font color=\"cyan\">problematic</font> in deep networks.\n",
    "- It squashes all values between 0 and 1 and when you do so repeatedly, neuron outputs and their <font color=\"orange\">gradients can vanish entirely</font>.\n",
    "- It was mentioned for historical reasons, but modern networks use the <font color=\"orange\">RELU (Rectified Linear Unit)</font>.<br>\n",
    "<img src=\"resource/NN_05.png\" width=\"350px\"><br>\n",
    "- RELU was shown to have better convergence properties almost everywhere.\n",
    "    - introduce non-linearity with just simple function\n",
    "    - making network sparse and efficient by sets any negative value to zero\n",
    "    - computationally cheaper compared to sigmoid or tanh\n",
    "    - maintains strong gradients during training, help deeper networks to converge effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Better Optimizer\n",
    "- <font color=\"orange\">Optimizer</font> help to minimizing the loss function by updating the neural network weights.\n",
    "- Those minimum value in parameter space is called <font color=\"orange\">'local minima'</font>,\n",
    "- <font color=\"orange\">SGD Optimizer</font> might not perform well on <font color=\"orange\">very high-dimensional spaces</font> like here ‚Äî we have on the order of 10K weights and biases.\n",
    "- <font color=\"orange\">\"saddle points\"</font> are frequent on that kind of network. \n",
    "- These are points that are not local minima, but where the <font color=\"orange\">gradient is zero</font> and the <font color=\"orange\">SGD optimizer</font> stays stuck there. \n",
    "- Open <font color=\"orange\">'2.2 neural_network_optimizer.ipynb'</font> in Google Colab to learn more...<br> \n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-Image-Classification/blob/main/Pertemuan%202/2.2%20neural_network_optimizer.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<br><br>\n",
    "<img src=\"resource/LocalMinima.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________\n",
    "<br><br><br>\n",
    "# Source\n",
    "- https://medium.com/analytics-vidhya/journey-of-gradient-descent-from-local-to-global-c851eba3d367\n",
    "- https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist#6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Dropout Regularization\n",
    "- Intro to Dropout Regularization \n",
    "- Experiment Handling Overfitting using Dropout Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è *Please open this notebook in Google Colab* by click below link ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è<br><br>\n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-Image-Classification/blob/main/Pertemuan%202/2.4%20dropout_regularization.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><br><br><br>\n",
    "- Click `Connect` button in top right Google Colab notebook,<br>\n",
    "<img src=\"resource/cl-connect-gpu.png\" width=\"250px\">\n",
    "- If connecting process completed, it will turn to something look like this<br>\n",
    "<img src=\"resource/cl-connect-gpu-success.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Intro to Dropout Regularization\n",
    "- <font color=\"orange\">Dropout</font> is one of the <font color=\"orange\">oldest</font> regularization techniques in deep learning. \n",
    "- At each training iteration, it <font color=\"orange\">drops random neurons</font> from the network with a probability <font color=\"orange\">p</font> (typically 25% to 50%). \n",
    "- In practice, <font color=\"orange\">neuron outputs</font> are set to <font color=\"orange\">0</font>. \n",
    "- The network result is that these neurons will <font color=\"orange\">not participate in the loss computation</font> this time around and they will <font color=\"orange\">not get weight updates</font>. \n",
    "- Different neurons will be dropped at each training iteration.<br><br>\n",
    "<img src=\"resource/Dropout_2.png\" width=\"700px\"><br><br>\n",
    "- When evaluation the performance of your network of course you put all the neurons back (dropout rate=0)\n",
    "    - This procedure <font color=\"orange\">slowdown</font> the model learn during <font color=\"orange\">training</font>.\n",
    "    - But use <font color=\"orange\">all knowledge</font> the model have, during <font color=\"orange\">evaluation</font>.\n",
    "    - This hopefully make training loss dropping in same portion with the validation loss and <font color=\"orange\">avoiding overfitting</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check GPU connected into Colab environment is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import gdown\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# clear output cell\n",
    "display.clear_output()\n",
    "\n",
    "print(f\"torch : {torch.__version__}\")\n",
    "print(f\"torch vision : {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'MNIST' # the dataset name\n",
    "DATASET_NUM_CLASS = 10 # number of class in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default using gdrive_id Dataset `mnist_dataset.zip` (1-FfwJrllyHofQwIbMb_IxAkxnfMGSFmR)\n",
    "gdrive_id = '1-FfwJrllyHofQwIbMb_IxAkxnfMGSFmR' # <-----  ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è USE YOUR OWN GDrive ID FOR CUSTOM DATASET ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "\n",
    "# download zip from GDrive\n",
    "url = f'https://drive.google.com/uc?id={gdrive_id}'\n",
    "gdown.download(url, DATASET_NAME + \".zip\", quiet=False)\n",
    "\n",
    "# unzip dataset\n",
    "!unzip {DATASET_NAME}.zip -d {DATASET_NAME}\n",
    "\n",
    "# clear output cell\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Dataset class\n",
    "# it's just helper to load image dataset using OpenCV and convert to pytorch tensor\n",
    "# also doing a label encoding using one-hot encoding\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = sorted([file for file in os.listdir(root_dir) if file.lower().endswith('.png')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read image from corresponding .png file\n",
    "        image_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = cv2.imread(image_path)  # Load image using OpenCV\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert BGR to GRAY\n",
    "        image = torch.from_numpy(image).to(torch.float32)  # Convert NumPy array to PyTorch tensor\n",
    "        image = torch.flatten(image) # flatten image from 2D Tensor (28x28) into a 1D tensor (784)\n",
    "\n",
    "        # Read label from corresponding .txt file\n",
    "        label_path = os.path.splitext(image_path)[0] + \".txt\"\n",
    "        with open(label_path, 'r') as label_file:\n",
    "            label = int(label_file.read().strip())  # Assuming labels are integers\n",
    "\n",
    "        # Apply one-hot encoding into label\n",
    "        labels_tensor = torch.tensor(label)\n",
    "        one_hot_encoded = F.one_hot(labels_tensor, num_classes=DATASET_NUM_CLASS).to(torch.float32)\n",
    "\n",
    "        return image, one_hot_encoded\n",
    "\n",
    "\n",
    "\n",
    "# instantiate dataset\n",
    "# in here the image dataset is not loaded yet\n",
    "# we only read all image files names in fataset folder\n",
    "all_train_dataset = CustomDataset(root_dir=f'{DATASET_NAME}/dataset/train')\n",
    "test_dataset = CustomDataset(root_dir=f'{DATASET_NAME}/dataset/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'all_train_dataset' into 'train' and 'validation' set using `random_split()` function\n",
    "train_dataset, validation_dataset = random_split(all_train_dataset, [50000, 10000])\n",
    "\n",
    "print(f\"Train Dataset : {len(train_dataset)} data\")\n",
    "print(f\"Validation Dataset : {len(validation_dataset)} data\")\n",
    "print(f\"Test Dataset : {len(test_dataset)} data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network with Dropout Layer here!!!\n",
    "- In pytorch we can append Dropout after Activation Function in between hidden layer\n",
    "- The following example to add dropout layer with probability <font color=\"orange\">p = 0.25 (25%)</font>\n",
    "    ```\n",
    "    nn.Dropout(0.25)\n",
    "    ```\n",
    "- More about [Pytorch Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=784, out_features=200),   # 1st hidden layer\n",
    "    nn.ReLU(),                                      # ReLU Activation function\n",
    "    nn.Dropout(0.25),                               # Dropout 25% neuron\n",
    "    nn.Linear(in_features=200, out_features=100),   # 2nd hidden layer\n",
    "    nn.ReLU(),                                      # ReLU Activation function\n",
    "    nn.Linear(in_features=100, out_features=60),    # 3rd hidden layer\n",
    "    nn.ReLU(),                                      # ReLU Activation function\n",
    "    nn.Dropout(0.25),                               # Dropout 25% neuron\n",
    "    nn.Linear(in_features=60, out_features=30),     # 4th hidden layer\n",
    "    nn.ReLU(),                                      # ReLU Activation function\n",
    "    nn.Linear(in_features=30, out_features=10),     # Output layer\n",
    "    nn.LogSoftmax(dim=1)                            # Log probabilities for classification\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting Optimizer, Learning Rate Scheduler & Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Setup optimizer, learning rate scheduler & loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Decay Learning Rate by a factor of 0.1 every 5 epochs\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To run training process, we can use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Add progress bar for training loop\n",
    "    progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs = inputs.to(device)  # Move inputs to device\n",
    "        labels = labels.to(device)  # Move labels to device\n",
    "\n",
    "        # Resets the gradients of all the model's parameters before the backward pass\n",
    "        optimizer.zero_grad()\n",
    "        # Pass input tensor to model\n",
    "        outputs = model(inputs)\n",
    "        # Calculate loss value\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # Computes the gradient of the loss with respect to each parameter in model\n",
    "        loss.backward()\n",
    "        # Adjust model parameters\n",
    "        optimizer.step()\n",
    "        # Sum loss value\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate correct & total prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels.argmax(1)).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Update progress bar description with current loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate average training loss\n",
    "    average_train_loss = running_loss / len(train_loader.dataset)\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    return average_train_loss, train_accuracy\n",
    "\n",
    "def validate(model, val_loader, loss_function):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Add progress bar for validation loop\n",
    "    progress_bar = tqdm(val_loader, desc='Validating', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device)  # Move inputs to device\n",
    "            labels = labels.to(device)  # Move labels to device\n",
    "\n",
    "            # Pass input tensor to model\n",
    "            outputs = model(inputs)\n",
    "            # Calculate loss value\n",
    "            loss = loss_function(outputs, labels)\n",
    "            # Sum loss value\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate correct & total prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels.argmax(1)).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            # Update progress bar description with loss\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    average_val_loss = running_loss / len(val_loader.dataset)\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "    return average_val_loss, val_accuracy\n",
    "\n",
    "# This is a training loop for selected Epoch\n",
    "# each epoch will process all training and validation set, chunked into small batch size data\n",
    "# then measure the loss & accuracy of training and validation set\n",
    "NUM_EPOCH = 10  # You can change this value\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCH}\")\n",
    "\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, loss_function)\n",
    "    val_loss, val_accuracy = validate(model, validation_loader, loss_function)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy * 100)  # Convert to percentage\n",
    "    val_accuracies.append(val_accuracy * 100)  # Convert to percentage\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step() # <--- ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è THIS LINE WILL SCHEDULED THE DECAY LEARNING RATE IN OPTIMIZER \n",
    "\n",
    "    print(f\"Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot Loss and Accuracy of Training vs Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Loss & Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, NUM_EPOCH + 1))\n",
    "\n",
    "# Plotting loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate Model, find Precision, Recal each class data, measure accuracy and compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# define evaluate function for test set\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Add progress bar for validation loop\n",
    "    progress_bar = tqdm(test_loader, desc='Evaluating', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # iterate over all batched test set\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device) # move inputs to device\n",
    "            labels = labels.to(device) # move labels to device\n",
    "\n",
    "            # pass input tensor to model\n",
    "            outputs = model(inputs)\n",
    "            # get prediction\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # collect all labels & preds\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Evaluation on test set\n",
    "all_labels, all_preds = evaluate(model, test_loader)\n",
    "all_labels = np.argmax(all_labels, axis=1)\n",
    "\n",
    "# Calculate classification report\n",
    "labels = [str(i) for i in range(DATASET_NUM_CLASS)]\n",
    "print(classification_report(all_labels, all_preds, target_names=labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'trained_model.pt')\n",
    "\n",
    "# Download the model file\n",
    "from google.colab import files\n",
    "files.download('trained_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    ">### Discussion\n",
    ">- Is adding Dropout Regularization solving overfitting in more bigger network?\n",
    ">- Try adding more Dropout in between all hidden layer with p = 0.25\n",
    ">   - It's working better? \n",
    ">- Try changing probability p from 0.25 to other value, e.g 0.5\n",
    ">\n",
    ">."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìùüìùüìù Special Care for Overfitting Problem\n",
    "\n",
    "- Overfitting happens when a neural network learns <font color=\"cyan\">\"badly\"</font>, \n",
    "- In a way that <font color=\"cyan\">works for the training set</font> but <font color=\"cyan\">not so well on validation set</font>. \n",
    "- There are regularisation techniques like dropout that can force it to learn in a better way but overfitting also has deeper roots.\n",
    "- Here are some root cause :<br><br>\n",
    "<img src=\"resource/Overfitting.png\" width=\"700px\"><br><br>\n",
    "- If you have <font color=\"cyan\">very little training data</font>, even a small network can learn it by heart and you will see \"overfitting\". \n",
    "    - Generally speaking, you always <font color=\"cyan\">need lots of data</font> to train neural networks.\n",
    "- If you already experimented with <font color=\"cyan\">different sizes</font> of network, <font color=\"cyan\">change optimizer</font>, <font color=\"cyan\">applied dropout</font> and <font color=\"cyan\">trained on lots of data</font> but still facing the overfitting, this means that <font color=\"cyan\">your neural network</font>, in its present shape, is <font color=\"orange\">not capable of extracting more information</font> from <font color=\"cyan\">your data</font>, as in our case here.\n",
    "- In the next chapter we will learn more about <font color=\"orange\">Convolution Neural Network (CNN)</font> that might more capable to learn good enough with dataset we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________\n",
    "<br><br><br>\n",
    "# Source\n",
    "- https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist#8\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

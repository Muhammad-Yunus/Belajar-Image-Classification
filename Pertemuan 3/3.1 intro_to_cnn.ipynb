{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Convolution Neural Network\n",
    "- Intro to Convolution Layer\n",
    "- Train Simple CNN using Pytorch on MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è *Please open this notebook in Google Colab* by click below link ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è<br><br>\n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-Image-Classification/blob/main/Pertemuan%203/3.1%20intro_to_cnn.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><br><br><br>\n",
    "- Click `Connect` button in top right Google Colab notebook,<br>\n",
    "<img src=\"resource/cl-connect-gpu.png\" width=\"250px\">\n",
    "- If connecting process completed, it will turn to something look like this<br>\n",
    "<img src=\"resource/cl-connect-gpu-success.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Into to Convolution Layer\n",
    "| Fully Connected Layer Neural Network | Convolution Neural Network |\n",
    "|-----------------------|----------------------------|\n",
    "|<img src=\"resource/MNIST_NN.gif\" width=\"500px\">|<img src=\"resource/MNIST_CNN.gif\" width=\"500px\">|\n",
    "\n",
    "\n",
    "#### 3.1.1.1 Convolution Operation\n",
    "- Convolutional neural networks apply a series of <font color=\"orange\">learnable filters</font> to the input image. \n",
    "- A convolutional layer is defined by the <font color=\"orange\">filter (or kernel) size</font>, the <font color=\"orange\">number of filters</font> applied and the <font color=\"orange\">stride</font>.\n",
    "- The input and the output of a convolutional layer each have <font color=\"orange\">three dimensions</font> (width, height, number of channels).\n",
    "- The <font color=\"orange\">depth of the output</font> (number of channels) is adjusted by using <font color=\"orange\">more or fewer filters</font>.\n",
    "- <font color=\"orange\">Padding</font> in a convolutional layer is like <font color=\"orange\">adding a border</font> of extra pixels around the edges of an image before applying the convolution operation. \n",
    "    - It helps <font color=\"orange\">maintain the original size</font> of the image after the convolution. <br><br>\n",
    "<img src=\"resource/CNN.gif\" width=\"500px\"><img src=\"resource/CNN2.png\" width=\"400px\"><br>\n",
    "<i>Illustration: filtering an image with <font color=\"cyan\">two successive filters</font> made of <font color=\"cyan\">4x4x3=48 learnable weights</font> each.</i><br><br>\n",
    "<img src=\"resource/CNN3.png\" width=\"600px\"><br>\n",
    "<i>Illustration: a convolutional neural network transforms <font color=\"cyan\">\"feature map\"</font> into <font color=\"cyan\">other \"feature map\"</font>.</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.2 Strided convolutions & Max Pooling Layer\n",
    "- When <font color=\"orange\">stacking convolutional layers</font>, the width and height of the output can be adjusted by using a <font color=\"orange\">stride >1</font> or with a <font color=\"orange\">max-pooling</font> operation.<br><br>\n",
    "- <font color=\"cyan\">Strided convolution</font>: by performing the convolutions with a <font color=\"orange\">stride</font> of 2 or 3, we can also <font color=\"orange\">shrink</font> the resulting feature map in its <font color=\"orange\">horizontal dimensions</font>.<br><br> \n",
    "- <font color=\"cyan\">Max pooling</font>: a <font color=\"orange\">sliding window</font> applying the <font color=\"orange\">MAX</font> operation (typically on 2x2 patches, repeated every 2 pixels).<br>\n",
    "    <img src=\"resource/CNN4.gif\" width=\"500px\"><br>\n",
    "    <i>Illustration: <font color=\"orange\">sliding</font> the computing window by <font color=\"orange\">3 pixels</font> results in <font color=\"orange\">fewer</font> output values.</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.3 Final Layer\n",
    "- After the last convolutional layer, the data is in the form of a \"feature map\". \n",
    "- There are <font color=\"orange\">two ways</font> of feeding it through the final layer :\n",
    "    1. <font color=\"cyan\">Flatten the feature map</font> into a <font color=\"orange\">1D Array</font> and then feed it to the <font color=\"orange\">softmax layer</font>.\n",
    "        - This kind of final layer similar to what we have done at the begining when creating a <font color=\"orange\">fully connected layer</font>.\n",
    "        - We are also able to add extra hidden layer between flattern featur map and softmax.\n",
    "    2. Apply <font color=\"cyan\">Global average pooling</font> to the feature map and then feed it to the <font color=\"orange\">softmax layer</font>.\n",
    "        - This technique <font color=\"orange\">more computationaly cheaper</font> compare to flattern feature map into 1D array.<br><br>\n",
    "<img src=\"resource/CNN5.png\" width=\"600px\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "_______________\n",
    "<br><br>\n",
    "### 3.1.2 Train Simple CNN using Pytorch on MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check GPU connected into Colab environment is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import gdown\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# clear output cell\n",
    "display.clear_output()\n",
    "\n",
    "print(f\"torch : {torch.__version__}\")\n",
    "print(f\"torch vision : {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'MNIST' # the dataset name\n",
    "DATASET_NUM_CLASS = 10 # number of class in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default using gdrive_id Dataset `mnist_dataset.zip` (1-FfwJrllyHofQwIbMb_IxAkxnfMGSFmR)\n",
    "gdrive_id = '1-FfwJrllyHofQwIbMb_IxAkxnfMGSFmR' # <-----  ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è USE YOUR OWN GDrive ID FOR CUSTOM DATASET ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "\n",
    "# download zip from GDrive\n",
    "url = f'https://drive.google.com/uc?id={gdrive_id}'\n",
    "gdown.download(url, DATASET_NAME + \".zip\", quiet=False)\n",
    "\n",
    "# unzip dataset\n",
    "!unzip {DATASET_NAME}.zip -d {DATASET_NAME}\n",
    "\n",
    "# clear output cell\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load MNIST Dataset\n",
    "    - <font color=\"orange\">DONT FLATTEN THE INPUT IMAGE ON DATA LOADER</font>,\n",
    "    - WE WILL FEED 2D 28x28 MNIST DIGIT IMAGE DATA INTO MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Dataset class\n",
    "# it's just helper to load image dataset using OpenCV and convert to pytorch tensor\n",
    "# also doing a label encoding using one-hot encoding\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = sorted([file for file in os.listdir(root_dir) if file.lower().endswith('.png')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read image from corresponding .png file\n",
    "        image_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = cv2.imread(image_path)  # Load image using OpenCV\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert BGR to GRAY\n",
    "        image = torch.from_numpy(image).to(torch.float32)  # Convert NumPy array to PyTorch tensor\n",
    "        image = image.view(1, 28, 28) # reshape loaded MNIST image into 1x28x28 format (required by the model)\n",
    "\n",
    "        # Read label from corresponding .txt file\n",
    "        label_path = os.path.splitext(image_path)[0] + \".txt\"\n",
    "        with open(label_path, 'r') as label_file:\n",
    "            label = int(label_file.read().strip())  # Assuming labels are integers\n",
    "\n",
    "        # Apply one-hot encoding into label\n",
    "        labels_tensor = torch.tensor(label)\n",
    "        one_hot_encoded = F.one_hot(labels_tensor, num_classes=DATASET_NUM_CLASS).to(torch.float32)\n",
    "\n",
    "        return image, one_hot_encoded\n",
    "\n",
    "\n",
    "\n",
    "# instantiate dataset\n",
    "# in here the image dataset is not loaded yet\n",
    "# we only read all image files names in fataset folder\n",
    "all_train_dataset = CustomDataset(root_dir=f'{DATASET_NAME}/dataset/train')\n",
    "test_dataset = CustomDataset(root_dir=f'{DATASET_NAME}/dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"All Train Dataset : {len(all_train_dataset)} data\")\n",
    "print(f\"Test Dataset : {len(test_dataset)} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'all_train_dataset' into 'train' and 'validation' set using `random_split()` function\n",
    "train_dataset, validation_dataset = random_split(all_train_dataset, [50000, 10000])\n",
    "\n",
    "print(f\"Train Dataset : {len(train_dataset)} data\")\n",
    "print(f\"Validation Dataset : {len(validation_dataset)} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2.1 Create a CNN Model \n",
    "- Let us build a convolutional network for handwritten digit recognition. \n",
    "- We will use :\n",
    "    - <font color=\"orange\">Three Convolutional Layers</font> at the top,  \n",
    "    - <font color=\"orange\">Flatten</font> the feature map of last convolution layer,\n",
    "    - Connect them with one <font color=\"orange\">Fully-connected Layer</font>,\n",
    "    - And <font color=\"orange\">Softmax Layer</font> at the end. <br><br>\n",
    "<img src=\"resource/CNN6.png\" width=\"600px\"><br><br>\n",
    "> ___\n",
    "> Notice that the <font color=\"orange\">second</font> and <font color=\"orange\">third</font> convolutional layers have a <font color=\"orange\">stride of two</font> which explains why they bring the number of output values down from <font color=\"orange\">28x28</font> to <font color=\"orange\">14x14</font> and then <font color=\"orange\">7x7</font>.\n",
    "> ___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is how to create pytorch 2D Convolution over an input data, \n",
    "    >`\n",
    "    >    - With square kernels and equal stride <br>\n",
    "    >    ```m = nn.Conv2d(in_channels=16, out_channels=33, kernel_size=3, stride=2)```<br><br>\n",
    "    >    - non-square kernels with stride and padding <br> \n",
    "    >    ```m = nn.Conv2d(in_channels=16, out_channels=33, kernel_size=(3, 5), stride=2, padding=2)```<br><br>\n",
    "    >    - non-square kernels and unequal stride with padding <br>\n",
    "    >    ```m = nn.Conv2d(in_channels=16, out_channels=33, kernel_size=(3, 5), stride=(2,2), padding=2)```\n",
    "    >\n",
    "    >`\n",
    "    - More about [Pytorch Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    # Convolutional layers\n",
    "    # the first convolution layer will process 2D 28x28 tensor data\n",
    "    # always use padding=2 to maintain output feature map not loosing the border \n",
    "    nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3, padding=2), \n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=12, out_channels=24, kernel_size=6, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=24, out_channels=32, kernel_size=6, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    # Flatten layer\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    nn.Linear(in_features=32 * 7 * 7, out_features=200),  # Adjust in_features based on flattened output\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=200, out_features=10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer, loss function & metric\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To run training process, we can use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Add progress bar for training loop\n",
    "    progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs = inputs.to(device) # move inputs to device\n",
    "        labels = labels.to(device) # move labels to device\n",
    "\n",
    "        # resets the gradients of all the model's parameters before the backward pass\n",
    "        optimizer.zero_grad()\n",
    "        # pass 2D 28x28 input tensor to CNN model\n",
    "        outputs = model(inputs)\n",
    "        # calc loss value\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # computes the gradient of the loss with respect to each parameter in model\n",
    "        loss.backward()\n",
    "        # adjust model parameter\n",
    "        optimizer.step()\n",
    "        # sum loss value\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate correct & total prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels.argmax(1)).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Update progress bar description with current loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate average training loss\n",
    "    average_train_loss = running_loss / len(train_loader.dataset)\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    return average_train_loss, train_accuracy\n",
    "\n",
    "def validate(model, val_loader, loss_function):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Add progress bar for validation loop\n",
    "    progress_bar = tqdm(val_loader, desc='Validating', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device) # move inputs to device\n",
    "            labels = labels.to(device) # move labels to device\n",
    "\n",
    "            # pass 2D 28x28 input tensor to CNN model\n",
    "            outputs = model(inputs)\n",
    "            # calc loss value\n",
    "            loss = loss_function(outputs, labels)\n",
    "            # sum loss value\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate correct & total prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels.argmax(1)).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            # Update progress bar description with loss\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    average_val_loss = running_loss / len(val_loader.dataset)\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "    return average_val_loss, val_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This is a training loop for selected Epoch\n",
    "# each epoch will process all training and validation set, chunked into small batch size data\n",
    "# then measure the loss & accuracy of training and validation set\n",
    "NUM_EPOCH = 10      # you can change this value\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCH}\")\n",
    "\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, loss_function)\n",
    "    val_loss, val_accuracy = validate(model, validation_loader, loss_function)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy * 100)  # convert to percentage\n",
    "    val_accuracies.append(val_accuracy * 100)  # convert to percentage\n",
    "\n",
    "    print(f\"Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot Loss and Accuracy of Training vs Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Loss & Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, NUM_EPOCH + 1))\n",
    "\n",
    "# Plotting loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate Model, find Precision, Recal each class data, measure accuracy and compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# define evaluate function for test set\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Add progress bar for validation loop\n",
    "    progress_bar = tqdm(test_loader, desc='Evaluating', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # iterate over all batched test set\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device) # move inputs to device\n",
    "            labels = labels.to(device) # move labels to device\n",
    "\n",
    "            # pass 2D 28x28 input tensor to CNN model\n",
    "            outputs = model(inputs)\n",
    "            # get prediction\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # collect all labels & preds\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Evaluation on test set\n",
    "all_labels, all_preds = evaluate(model, test_loader)\n",
    "all_labels = np.argmax(all_labels, axis=1)\n",
    "\n",
    "# Calculate classification report\n",
    "labels = [str(i) for i in range(DATASET_NUM_CLASS)]\n",
    "print(classification_report(all_labels, all_preds, target_names=labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'trained_cnn_model.pt')\n",
    "\n",
    "# Download the model file\n",
    "from google.colab import files\n",
    "files.download('trained_cnn_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    ">## Discussion\n",
    ">- Now the model performance, is <font color=\"orange\">more better and achieve ~98% accuracy</font>,\n",
    "><img src=\"resource/CNN7.png\" width=\"800px\">\n",
    ">- But look at the <font color=\"orange\">validation loss</font> curve. it's look like a sign of <font color=\"orange\">overfitting</font>.\n",
    ">- Remember how to tackle it? yes, now we will combine <font color=\"orange\">Convolution</font> with <font color=\"orange\">Dropout</font> Regularization to handle this.\n",
    ">   - Try <font color=\"cyan\">add droput layer</font> in between fully connected layer 200 & 60 with probability 40% (p=0.4)<br>\n",
    ">      `nn.Dropout(0.4)`\n",
    ">\n",
    ">."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìùüìùüìù Other Regularization Technique\n",
    "\n",
    "- Now we will try to learn new possibility to adopt other regularization technique.\n",
    "- It's called <font color=\"cyan\">Batch Normalization</font>,<br><br>\n",
    "<img src=\"resource/BatchNorm2.png\" width=\"500px\"><br><br>\n",
    "- Open <font color=\"orange\">'3.2 cnn_with_batch_normalization.ipynb'</font> in Google Colab to learn more...<br> \n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-Image-Classification/blob/main/Pertemuan%203/3.2%20cnn_with_batch_normalization.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________\n",
    "<br><br><br>\n",
    "# Source\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "- https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist#9\n",
    "- https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist#10"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
